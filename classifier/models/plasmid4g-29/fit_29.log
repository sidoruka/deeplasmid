Using TensorFlow backend.
deep-libs1 imported elaT=0.0 sec
deep-libs2 imported, TF.ver=1.3.0, elaT=0.0 sec
myArg: arrIdx 29
myArg: verb 0
myArg: prjName assayer4
myArg: dataPath data
myArg: outPath ./
myArg: seedModel None
myArg: seedWeights None
myArg: kfoldOffset 4
myArg: noXterm True
myArg: epochs 30
myArg: batch_size 200
myArg: events 601000
myArg: dropFrac 0.1
myArg: earlyStopPatience 10
myArg: checkPtOn True
myArg: reduceLearn True
disable Xterm
Plotter_Plasmid : Graphics started
Deep_Plasmid , prj: assayer4
oneHot base, size= 15 , sample:
base: A 1-hot: [1.0, 0.0, 0.0, 0.0]
base: C 1-hot: [0.0, 1.0, 0.0, 0.0]
base: T 1-hot: [0.0, 0.0, 1.0, 0.0]
base: G 1-hot: [0.0, 0.0, 0.0, 1.0]
all bases : ['A', 'C', 'T', 'G', 'N', 'Y', 'K', 'M', 'V', 'S', 'H', 'R', 'W', 'B', 'D']
use seqLenCut= 300
Deep_Plasmid , prj: assayer4
globFeatureL 10 fixed order: ['gc_content', 'len_sequence', 'A_longestHomopol', 'A_totalLongHomopol', 'C_longestHomopol', 'C_totalLongHomopol', 'T_longestHomopol', 'T_totalLongHomopol', 'G_longestHomopol', 'G_totalLongHomopol']

prep_labeled scaffolds from: data/assayer4.plasm-scaff-split.yml
  read  yaml: data/assayer4.plasm-scaff-split.yml  size=7   elaT=0.7 sec
  role: plasm segL: 3  dom: val , numSpec: 156 , kfoldOffset= 4
sum3= 99998  numScaff= 156
   request  numSamples= 37562 val plasm
1 scaff: NC_000948.1 plasm size/B=30750 numSampl=244 got=244=244
2 scaff: NC_000951.1 plasm size/B=29838 numSampl=242 got=242=242
3 scaff: NC_000952.1 plasm size/B=30800 numSampl=245 got=245=245
4 scaff: NC_000953.1 plasm size/B=30885 numSampl=245 got=245=245
   completed role= plasm ,got numSamples= 37558
prep: val plasm  completed, elaT=0.9 sec , gotSamples= 37558

prep_labeled scaffolds from: data/assayer4.plasm-scaff-split.yml
  read  yaml: data/assayer4.plasm-scaff-split.yml  size=7   elaT=0.7 sec
  role: plasm segL: [4, 0, 1, 2]  dom: train , numSpec: 653 , kfoldOffset= 4
sum3= 99997  numScaff= 653
   request  numSamples= 300500 train plasm
1 scaff: NC_000906.2 plasm size/B=8506 numSampl=298 got=298=298
2 scaff: NC_000956.1 plasm size/B=52971 numSampl=607 got=607=607
3 scaff: NC_001390.1 plasm size/B=2355 numSampl=201 got=201=201
4 scaff: NC_001476.1 plasm size/B=12887 numSampl=346 got=346=346
500 scaff: NC_002150.1 plasm size/B=6159 numSampl=268 got=268=268
   completed role= plasm ,got numSamples= 300467
prep: train plasm  completed, elaT=1.5 sec , gotSamples= 300467

prep_labeled scaffolds from: data/assayer4.main-scaff-split.yml
  read  yaml: data/assayer4.main-scaff-split.yml  size=7   elaT=7.5 sec
  role: main segL: 3  dom: val , numSpec: 1696 , kfoldOffset= 4
sum3= 99994  numScaff= 1696
   request  numSamples= 37562 val main
1 scaff: NZ_AAAL02000004.1 main size/B=201792 numSampl=55 got=55=55
2 scaff: NZ_AARK02000187.1 main size/B=5644 numSampl=13 got=13=13
3 scaff: NZ_ABBE01001076.1 main size/B=10660 numSampl=16 got=16=16
4 scaff: NZ_ABKZ01000384.1 main size/B=19628 numSampl=20 got=20=20
500 scaff: NZ_CSOA01000080.1 main size/B=17966 numSampl=20 got=20=20
1000 scaff: NZ_JUVB01000066.1 main size/B=84763 numSampl=37 got=37=37
1500 scaff: NZ_MMUK01000017.1 main size/B=231662 numSampl=59 got=59=59
   completed role= main ,got numSamples= 37475
prep: val main  completed, elaT=8.6 sec , gotSamples= 37475

prep_labeled scaffolds from: data/assayer4.main-scaff-split.yml
  read  yaml: data/assayer4.main-scaff-split.yml  size=7   elaT=7.3 sec
  role: main segL: [4, 0, 1, 2]  dom: train , numSpec: 6610 , kfoldOffset= 4
sum3= 100015  numScaff= 6610
   request  numSamples= 300500 train main
1 scaff: NZ_AAAM04000255.1 main size/B=2851 numSampl=21 got=21=21
2 scaff: NZ_AAMD01000365.1 main size/B=2826 numSampl=21 got=21=21
3 scaff: NZ_AAQF01000075.1 main size/B=21478 numSampl=42 got=42=42
4 scaff: NZ_AARK02000283.1 main size/B=3096 numSampl=22 got=22=22
500 scaff: NZ_CTFN01000092.1 main size/B=56898 numSampl=63 got=63=63
1000 scaff: NZ_JWAS01000124.1 main size/B=2627 numSampl=21 got=21=21
1500 scaff: NZ_MUOK01000010.1 main size/B=4699 numSampl=24 got=24=24
2000 scaff: NZ_CFUZ01000056.1 main size/B=4583 numSampl=24 got=24=24
2500 scaff: NZ_JMNT01000081.1 main size/B=16957 numSampl=38 got=38=38
3000 scaff: NZ_LZKW01000108.1 main size/B=4171 numSampl=24 got=24=24
3500 scaff: NZ_AZRA01000105.1 main size/B=14041 numSampl=36 got=36=36
4000 scaff: NZ_FRUK01000126.1 main size/B=64450 numSampl=66 got=66=66
4500 scaff: NZ_LLAM01000156.1 main size/B=8400 numSampl=30 got=30=30
5000 scaff: NZ_AINB01000061.1 main size/B=20084 numSampl=41 got=41=41
5500 scaff: NZ_CXFU01000066.1 main size/B=22272 numSampl=43 got=43=43
6000 scaff: NZ_KK323831.1 main size/B=22352 numSampl=43 got=43=43
6500 scaff: NZ_NACS01000124.1 main size/B=53030 numSampl=61 got=61=61
   completed role= main ,got numSamples= 299803
prep: train main  completed, elaT=12.7 sec , gotSamples= 299803
   build_training_data: val plasm  numSeq=37558 ...
   build_training_data: val main  numSeq=37475 ...
build_training_data for dom: val Xs,Xf,Y: (75033, 300, 4) (75033, 10) (75033,) SNR=1.002 done
 add noise  0.01  to gloft in val
   build_training_data: train plasm  numSeq=300467 ...
   build_training_data: train main  numSeq=299803 ...
build_training_data for dom: train Xs,Xf,Y: (600270, 300, 4) (600270, 10) (600270,) SNR=1.002 done
 add noise  0.05  to gloft in train
start fresh model
build_model inpA: (?, 300, 4)   inpB: (?, 10)
Dens act= relu  recurDropFrac= 0.05  layerDropFrac= 0.1  idx= 29
 (re)Compile model
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
seq_300_x_4 (InputLayer)         (None, 300, 4)        0                                            
____________________________________________________________________________________________________
globF_10 (InputLayer)            (None, 10)            0                                            
____________________________________________________________________________________________________
A_40 (LSTM)                      (None, 300, 40)       7200        seq_300_x_4[0][0]                
____________________________________________________________________________________________________
G_4 (Dense)                      (None, 4)             44          globF_10[0][0]                   
____________________________________________________________________________________________________
B_40 (LSTM)                      (None, 40)            12960       A_40[0][0]                       
____________________________________________________________________________________________________
H_4 (Dense)                      (None, 4)             20          G_4[0][0]                        
____________________________________________________________________________________________________
seq_glob (Concatenate)           (None, 44)            0           B_40[0][0]                       
                                                                   H_4[0][0]                        
____________________________________________________________________________________________________
C_40 (Dense)                     (None, 40)            1800        seq_glob[0][0]                   
____________________________________________________________________________________________________
fr_0.1 (Dropout)                 (None, 40)            0           C_40[0][0]                       
____________________________________________________________________________________________________
D_20 (Dense)                     (None, 20)            820         fr_0.1[0][0]                     
____________________________________________________________________________________________________
fr_same (Dropout)                (None, 20)            0           D_20[0][0]                       
____________________________________________________________________________________________________
score (Dense)                    (None, 1)             21          fr_same[0][0]                    
====================================================================================================
Total params: 22,865
Trainable params: 22,865
Non-trainable params: 0
____________________________________________________________________________________________________
model (re)compiled elaT=0.0 sec
save model full to .//assayer4.model.h5
  closed  hdf5: .//assayer4.model.h5  size=0.12 MB
train for epochs: 30
enabled EarlyStopping, patience= 10
enabled ModelCheckpoint, period= 1
enabled ReduceLROnPlateau

Train_model X: (600270, 300, 4)  earlyStop= 10  epochs= 30  batch= 200 , data SNR(train)=1.0022, SNR(valid)=1.0022
Train on 600270 samples, validate on 75033 samples
Epoch 1/30
Epoch 00000: val_loss improved from inf to 0.53037, saving model to .//assayer4.weights_best.h5
1561s - loss: 0.5904 - acc: 0.6873 - val_loss: 0.5304 - val_acc: 0.7440
Epoch 2/30
Epoch 00001: val_loss improved from 0.53037 to 0.52490, saving model to .//assayer4.weights_best.h5
1535s - loss: 0.5735 - acc: 0.7020 - val_loss: 0.5249 - val_acc: 0.7515
Epoch 3/30
Epoch 00002: val_loss did not improve
1535s - loss: 0.5683 - acc: 0.7050 - val_loss: 0.5268 - val_acc: 0.7419
Epoch 4/30
Epoch 00003: val_loss improved from 0.52490 to 0.51426, saving model to .//assayer4.weights_best.h5
1535s - loss: 0.5630 - acc: 0.7092 - val_loss: 0.5143 - val_acc: 0.7501
Epoch 5/30
Epoch 00004: val_loss did not improve
1535s - loss: 0.5558 - acc: 0.7136 - val_loss: 0.5210 - val_acc: 0.7390
Epoch 6/30
Epoch 00005: val_loss did not improve
1535s - loss: 0.5546 - acc: 0.7143 - val_loss: 0.5208 - val_acc: 0.7409
Epoch 7/30
Epoch 00006: val_loss did not improve
1535s - loss: 0.5511 - acc: 0.7171 - val_loss: 0.5166 - val_acc: 0.7436
Epoch 8/30
Epoch 00007: val_loss improved from 0.51426 to 0.51283, saving model to .//assayer4.weights_best.h5
1535s - loss: 0.5460 - acc: 0.7207 - val_loss: 0.5128 - val_acc: 0.7427
Epoch 9/30
Epoch 00008: val_loss improved from 0.51283 to 0.50274, saving model to .//assayer4.weights_best.h5
1536s - loss: 0.5378 - acc: 0.7268 - val_loss: 0.5027 - val_acc: 0.7463
Epoch 10/30
Epoch 00009: val_loss improved from 0.50274 to 0.49177, saving model to .//assayer4.weights_best.h5
1536s - loss: 0.5259 - acc: 0.7353 - val_loss: 0.4918 - val_acc: 0.7596
Epoch 11/30
Epoch 00010: val_loss improved from 0.49177 to 0.47999, saving model to .//assayer4.weights_best.h5
1535s - loss: 0.5144 - acc: 0.7429 - val_loss: 0.4800 - val_acc: 0.7670
Epoch 12/30
Epoch 00011: val_loss improved from 0.47999 to 0.46704, saving model to .//assayer4.weights_best.h5
1536s - loss: 0.5064 - acc: 0.7489 - val_loss: 0.4670 - val_acc: 0.7794
Epoch 13/30
Epoch 00012: val_loss improved from 0.46704 to 0.45633, saving model to .//assayer4.weights_best.h5
1537s - loss: 0.4986 - acc: 0.7542 - val_loss: 0.4563 - val_acc: 0.7855
Epoch 14/30
Epoch 00013: val_loss did not improve
1537s - loss: 0.4923 - acc: 0.7587 - val_loss: 0.4577 - val_acc: 0.7824
Epoch 15/30
Epoch 00014: val_loss did not improve
1535s - loss: 0.4870 - acc: 0.7628 - val_loss: 0.4606 - val_acc: 0.7838
Epoch 16/30
Epoch 00015: val_loss improved from 0.45633 to 0.45274, saving model to .//assayer4.weights_best.h5
1536s - loss: 0.4822 - acc: 0.7660 - val_loss: 0.4527 - val_acc: 0.7876
Epoch 17/30
Epoch 00016: val_loss improved from 0.45274 to 0.44043, saving model to .//assayer4.weights_best.h5
1536s - loss: 0.4779 - acc: 0.7687 - val_loss: 0.4404 - val_acc: 0.7949
Epoch 18/30
Epoch 00017: val_loss did not improve
1534s - loss: 0.4743 - acc: 0.7708 - val_loss: 0.4455 - val_acc: 0.7923
Epoch 19/30
Epoch 00018: val_loss did not improve
1535s - loss: 0.4712 - acc: 0.7724 - val_loss: 0.4513 - val_acc: 0.7852
Epoch 20/30
Epoch 00019: val_loss did not improve
1535s - loss: 0.4686 - acc: 0.7745 - val_loss: 0.4442 - val_acc: 0.7985
Epoch 21/30
Epoch 00020: val_loss improved from 0.44043 to 0.43928, saving model to .//assayer4.weights_best.h5
1536s - loss: 0.4653 - acc: 0.7767 - val_loss: 0.4393 - val_acc: 0.7948
Epoch 22/30
Epoch 00021: val_loss did not improve

Epoch 00021: reducing learning rate to 0.0003000000142492354.
1538s - loss: 0.4629 - acc: 0.7785 - val_loss: 0.4396 - val_acc: 0.8028
Epoch 23/30
Epoch 00022: val_loss improved from 0.43928 to 0.43519, saving model to .//assayer4.weights_best.h5
1535s - loss: 0.4557 - acc: 0.7826 - val_loss: 0.4352 - val_acc: 0.7970
Epoch 24/30
Epoch 00023: val_loss improved from 0.43519 to 0.43355, saving model to .//assayer4.weights_best.h5
1536s - loss: 0.4550 - acc: 0.7831 - val_loss: 0.4336 - val_acc: 0.8015
Epoch 25/30
Epoch 00024: val_loss improved from 0.43355 to 0.43273, saving model to .//assayer4.weights_best.h5
1537s - loss: 0.4534 - acc: 0.7837 - val_loss: 0.4327 - val_acc: 0.8020
Epoch 26/30
Epoch 00025: val_loss did not improve
1537s - loss: 0.4528 - acc: 0.7844 - val_loss: 0.4356 - val_acc: 0.7998
Epoch 27/30
Epoch 00026: val_loss did not improve
1536s - loss: 0.4523 - acc: 0.7848 - val_loss: 0.4329 - val_acc: 0.8004
Epoch 28/30
Epoch 00027: val_loss improved from 0.43273 to 0.43070, saving model to .//assayer4.weights_best.h5
1537s - loss: 0.4522 - acc: 0.7845 - val_loss: 0.4307 - val_acc: 0.8022
Epoch 29/30
Epoch 00028: val_loss improved from 0.43070 to 0.42895, saving model to .//assayer4.weights_best.h5
1537s - loss: 0.4505 - acc: 0.7856 - val_loss: 0.4289 - val_acc: 0.8005
Epoch 30/30
Epoch 00029: val_loss did not improve
1537s - loss: 0.4502 - acc: 0.7854 - val_loss: 0.4344 - val_acc: 0.7985

 Validation Acc:0.744 -->0.799 , end-loss:0.434  30 epochs, idx=29 , fit time=770.4 min
save model full to .//assayer4.model.h5
  closed  hdf5: .//assayer4.model.h5  size=0.31 MB
  closed  yaml: .//assayer4.history.yml  size=0.00 MB   elaT=0.0 sec
sss 30 30 30
sss2 30
Produce AUC of ROC, domain= val Y shape (75033,) [ 0.  0.  0.  0.  0.  0.]
AUC: 0.882388 , save: .//val.AUC.csv
found fpr= 0.100013342228 , tpr= 0.615607859843 , LR+= 6.15525734996
Graphics saving to .//idx29_assayer4_train_f10 PDF ...
	Command being timed: "python -u ./train_Plasmid.py --epochs 30 --events 601000 --arrIdx 29 --checkPt --kfoldOffset 4 --verbosity 0 --noXterm --reduceLr --outPath ./"
	User time (seconds): 448510.17
	System time (seconds): 133994.20
	Percent of CPU this job got: 1256%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 12:52:48
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 5638704
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 14502
	Minor (reclaiming a frame) page faults: 14676375863
	Voluntary context switches: 7630826267
	Involuntary context switches: 12864631
	Swaps: 0
	File system inputs: 0
	File system outputs: 6168
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
0e9381d335e4c60cb1427cc503b53562  train_Plasmid.py
it was JID 11722379
