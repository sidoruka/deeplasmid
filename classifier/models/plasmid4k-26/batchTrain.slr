#!/bin/bash -l

#SBATCH --nodes=1  --qos=genepool  -C haswell -A fungalp  
#SBATCH   --time=72:00:00  # -J plasm-reg  
#-SBATCH --partition=debug --time=00:29:00 -J plasm-dbg

#-SBATCH  -C haswell  
#  # Edison, 59 GB RAM

#SBATCH --array=18-29

arrIdx=${SLURM_ARRAY_TASK_ID}

#zipDim=${arrIdx} 
#zipDim=$[ 16+ 8 * ${arrIdx} ]
#zipDim=$(( ${arrIdx} % 3 +1 ))
#zipDim=$(( ${arrIdx} / 3 + 1 ))
#nCpu=$[ $SLURM_CPUS_ON_NODE / 4 ]
#dropFrac=0.${arrIdx}

kfoldOff=$(( ${arrIdx} % 6 ))

taskExe=train_Plasmid.py ; coreN='plasmid4k'

codeList="Deep_Plasmid.py  Oracle_Plasmid.py Plotter_Plasmid.py  batchTrain.slr Util_Plasmid.py  $taskExe"
dataDir="data_len_500-2.1M"

date
echo fit-`hostname`-arrIdx-$arrIdx start-jid ${SLURM_JOBID}
echo SLURM_CLUSTER_NAME=$SLURM_CLUSTER_NAME kfoldOff=$kfoldOff
md5sum $taskExe
#env|grep SLURM

srcDir=`pwd`
wrkDir=$CSCRATCH/${coreN}-${arrIdx}
mkdir -p $wrkDir
cp -rp $codeList  $wrkDir
cp -rp $dataDir ${wrkDir}/data
cd  $wrkDir
echo PWD=`pwd`
ls -l  $dataH5
module load python/3.6-anaconda-4.4

( sleep 180; date; free -g; top ibn1)&
( sleep 1200; date;  free -g; top ibn1)&

/usr/bin/time -v python -u ./$taskExe    --epochs 20 --events 601000 --arrIdx $arrIdx  --checkPt --kfoldOffset $kfoldOff --verbosity 0  --noXterm  --reduceLr  --outPath ./   >&fit_${arrIdx}.log

# attic: 
#seedModel=/global/cscratch1/sd/balewski/kinase9_sum/9a/kinase9a-${arrIdx}
# re-train: --seedModel $seedModel

md5sum $taskExe >>fit_${arrIdx}.log
echo it was JID  ${SLURM_JOBID} >>fit_${arrIdx}.log
echo done-`date`

# mv slurm log to final destination - it is alwasy a job-array
mv $srcDir/slurm-${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}.out .

