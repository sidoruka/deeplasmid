#!/bin/bash -l

#SBATCH --nodes=1  --qos=genepool  -C haswell -A fnglasmb
#SBATCH   --time=72:00:00  # -J plasm-reg  
#-SBATCH --partition=debug --time=00:29:00 -J plasm-dbg

#-SBATCH  -C haswell  
#  # Edison, 59 GB RAM

#SBATCH --array=18-29

arrIdx=${SLURM_ARRAY_TASK_ID}

#zipDim=${arrIdx} 
#zipDim=$[ 16+ 8 * ${arrIdx} ]
#zipDim=$(( ${arrIdx} % 3 +1 ))
#zipDim=$(( ${arrIdx} / 3 + 1 ))
#nCpu=$[ $SLURM_CPUS_ON_NODE / 4 ]
#dropFrac=0.${arrIdx}

kfoldOff=$(( ${arrIdx} % 6 ))

taskExe=train_Plasmid.py
coreN='plasmid4v'

codeList="Constants.py DL_Model.py Plotter_Plasmid.py  batchTrain.slr Util_Plasmid.py  $taskExe"
dataDir="delplasmid_data4train"

date
echo fit-`hostname`-arrIdx-$arrIdx start-jid ${SLURM_JOBID}
echo SLURM_CLUSTER_NAME=$SLURM_CLUSTER_NAME kfoldOff=$kfoldOff
md5sum $taskExe
#env|grep SLURM

srcDir=`pwd`
wrkDir=$CSCRATCH/${coreN}-${arrIdx}
mkdir -p $wrkDir
cp -rp $codeList  $wrkDir
cp -rp $dataDir ${wrkDir}/data
cd  $wrkDir
echo PWD=`pwd`
ls -l  $dataH5
module load python/3.6-anaconda-5.2

( sleep 180; date; free -g; top ibn1)&
( sleep 1200; date;  free -g; top ibn1)&

/usr/bin/time -v python -u ./$taskExe     --arrIdx $arrIdx   --kfoldOffset $kfoldOff --verbosity 0  --noXterm   --outPath ./   >&fit_${arrIdx}.log

# re-train: --seedModel $seedModel

md5sum $taskExe >>fit_${arrIdx}.log
echo it was JID  ${SLURM_JOBID} >>fit_${arrIdx}.log
echo done-`date`

# mv slurm log to final destination - it is alwasy a job-array
mv $srcDir/slurm-${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}.out .

